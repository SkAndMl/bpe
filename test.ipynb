{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6529d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import os\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "class BPE:\n",
    "\n",
    "    CHUNK_PAT = re.compile(r\"\"\"'s|'ve|'ll|'d|'t|'re|'m| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def __init__(self, tokenizer_dir: str, special_tokens: Optional[List[str]]=None):\n",
    "        \n",
    "        assert os.path.exists(tokenizer_dir)\n",
    "        tokenizer_path = Path(tokenizer_dir)\n",
    "        with open(tokenizer_path / \"merges.json\", \"r\") as f:\n",
    "            self.merges = {}\n",
    "            for k, v in json.loads(f.read()).items():\n",
    "                tok0, tok1 = k.split(\",\")\n",
    "                self.merges[(int(tok0), int(tok1))] = int(v)\n",
    "        \n",
    "        with open(tokenizer_path / \"vocab.json\", \"r\") as f:\n",
    "            self.vocab = {}\n",
    "            for k, v in json.loads(f.read()).items():\n",
    "                self.vocab[int(k)] = bytes(v)\n",
    "        \n",
    "        self.special_tokens = special_tokens\n",
    "        if special_tokens is not None:\n",
    "            self._init_special_tokens()\n",
    "    \n",
    "    def _init_special_tokens(self):\n",
    "        self.SPECIAL_PAT = r\"(\" + \\\n",
    "                           r\"|\".join(re.escape(sp_token) for sp_token in self.special_tokens) + \\\n",
    "                           r\")\" \n",
    "        self.sp_token_to_id = {self.special_tokens[i]: len(self.vocab)+i for i in range(len(self.special_tokens))}\n",
    "        self.id_to_sp_token = {v: k for k, v in self.sp_token_to_id.items()}\n",
    "\n",
    "    def encode_ordinary(self, text: str) -> List[int]:\n",
    "        byte_chunks = re.findall(self.CHUNK_PAT, text)\n",
    "        byte_chunks = [list(chunk.encode(\"utf-8\")) for chunk in byte_chunks]\n",
    "        while True:\n",
    "            freq = BPE.get_freq(byte_chunks)\n",
    "            if len(freq) == 0:\n",
    "                break\n",
    "            pair = min(freq, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "\n",
    "            byte_chunks = BPE.merge(byte_chunks, pair, self.merges[pair])\n",
    "        \n",
    "        return [token for chunk in byte_chunks for token in chunk]\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        if self.special_tokens is not None:\n",
    "            chunks = re.split(self.SPECIAL_PAT, text)\n",
    "        \n",
    "        tokens = []\n",
    "        for chunk in chunks:\n",
    "            if chunk in self.special_tokens:\n",
    "                tokens.append(self.sp_token_to_id[chunk])\n",
    "            else:\n",
    "                tokens.extend(self.encode_ordinary(chunk))\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        decoded_str = \"\"\n",
    "        _bytes = b\"\"\n",
    "        for token in tokens:\n",
    "            if token in self.id_to_sp_token:\n",
    "                decoded_str += _bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "                decoded_str += self.id_to_sp_token[token]\n",
    "                _bytes = b\"\"\n",
    "            else:\n",
    "                _bytes += self.vocab[token]\n",
    "        \n",
    "        if _bytes:\n",
    "            decoded_str += _bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        return decoded_str\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_freq(byte_chunks: List[List[int]]) -> dict:\n",
    "        freq = defaultdict(int)\n",
    "        for byte_chunk in byte_chunks:\n",
    "            for b0, b1 in zip(byte_chunk[:-1], byte_chunk[1:]):\n",
    "                freq[(b0, b1)] += 1\n",
    "        return freq\n",
    "\n",
    "    @staticmethod\n",
    "    def merge(byte_chunks: List[List[int]], pair: Tuple[int], new_id: int):\n",
    "        new_byte_chunks = []\n",
    "        for byte_chunk in byte_chunks:\n",
    "            new_byte_chunk = []\n",
    "            i = 0\n",
    "            while i < len(byte_chunk)-1:\n",
    "                if byte_chunk[i] == pair[0] and byte_chunk[i+1] == pair[1]:\n",
    "                    new_byte_chunk.append(new_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_byte_chunk.append(byte_chunk[i])\n",
    "                    i += 1\n",
    "            \n",
    "            if i == len(byte_chunk)-1:\n",
    "                new_byte_chunk.append(byte_chunk[-1])\n",
    "\n",
    "            new_byte_chunks.append(new_byte_chunk)\n",
    "        return new_byte_chunks\n",
    "\n",
    "    @classmethod\n",
    "    def build_vocab(cls, merges: Dict[Tuple[int], int]) -> Dict:\n",
    "        vocab = {_id: bytes([_id]) for _id in range(256)}\n",
    "        for (tok0, tok1), _id in sorted(merges.items(), key=lambda m: m[1]):\n",
    "            vocab[_id] = vocab[tok0] + vocab[tok1]\n",
    "        return vocab\n",
    "\n",
    "    @classmethod\n",
    "    def train(cls, filepath: str, num_merges: int, save_dir: str) -> None:\n",
    "\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        chunks = re.findall(cls.CHUNK_PAT, text)\n",
    "        byte_chunks = [list(bytes(chunk.encode(\"utf-8\"))) for chunk in chunks]\n",
    "        \n",
    "        merges = {} # (tok_id1, tok_id2) -> new_tok_id\n",
    "        for i in range(num_merges):\n",
    "            freq = cls.get_freq(byte_chunks)\n",
    "            if len(freq) == 0:\n",
    "                break\n",
    "            pair = max(freq, key=freq.get)\n",
    "            byte_chunks = cls.merge(byte_chunks, pair, 256 + i)\n",
    "            merges[pair] = 256 + i\n",
    "        \n",
    "        vocab = cls.build_vocab(merges)\n",
    "        \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        merges_path = os.path.join(save_dir, \"merges.json\")\n",
    "        vocab_path = os.path.join(save_dir, \"vocab.json\")\n",
    "        \n",
    "\n",
    "        with open(merges_path, \"w\") as f:\n",
    "            merges = {\",\".join([str(tok) for tok in k]): v for k, v in merges.items()}\n",
    "            f.write(json.dumps(merges))\n",
    "\n",
    "        with open(vocab_path, \"w\") as f:\n",
    "            vocab = {str(k): list(v) for k, v in vocab.items()}\n",
    "            f.write(json.dumps(vocab))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc88a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE.train(\"data/tinystories/train.txt\", 20, \"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62b3356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPE(\"tokenizer\", special_tokens=[\"<a>\", \"</a>\", \"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b9ef017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<a>hi how are you?</a><|endoftext|>'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"<a>hi how are you?</a><|endoftext|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b79d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
